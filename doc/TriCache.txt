
TriCache design
===============

TriCache is an experimental cache implementation which mimics the way Infinispan's Triangle approach works. Supported
operations are mainly put() and get().

Rebalancing on a view change is unsupported, as the main goal of TriCache is to compare steady-state operation
performance against Infinispan.

There's a fixed replication count of 2, and if a given key K maps to node B, then the backup owner for K will be C
(in A,B,C,D). In this group, all keys stored by primary (e.g.) D will also be present on backup node A. Having the
next-in-line member as backup node favors message- and data-batching (see below).

PUTs
----

- An originator (say C) does a put(K,V).
- The consistent hash of K points to A as primary and B as backup for K.
- C now sends a PUT(K,V) to A and blocks for an ACK
- A receives PUT, applies the change to its local store (hashmap) and sends a BACKUP message to backup node B
- B receives BACKUP, applies the change and sends an ACK to the original sender C.
- C receives the ACK and the PUT call returns
- Because the path of a PUT goes from originator to primary to backup back to originator, this is called the 'Triangle'
  approach in Infinispan
- There are 2 optimizations when originator == primary or originator == backup

GETs
----
- A GET for key K determines the primary and backup nodes for K, e.g. B and C. If the originator is B or C, then GET
  returns the value from the local hashmap.
- Else a GET request is sent to the primary and we block until an ACK has been received.
- The primary receives the GET and sends an ACK with the result back to the originator.
- The originator receives the ACK and unblocks the caller of GET with the return value.


OOB messages
------------

TriCache does *not* use any OOB messages. The reason is that OOB messages are assigned a separate thread at the receiver,
which sheperds them up the stack until they are delivered to the application. This means that if we (D) receive (at the
same time) 10 messages from A, 5 from B and 7 from C, we may have up to 22 threads delivering these messages.

Creating many threads increases costs for context switching and slows a system down.

Using regular messages has the following advantages:

- Only 3 threads are used in the above example to deliver the first message from A, B and C. An additional thread may be
  created to add the other messages from A, B and C to their respective tables in UNICAST3. After adding the messages,
  this thread is idle again. Fewer threads -> reduced context switching costs.

- When a messages (e.g. from A) has been delivered, the same thread tries to remove as many messages from A's table as
  possible. In this case, 9 messages are removed and delivered to the application *as a message batch*. Message batching
  has additional advantages, see below.


Message batches
---------------

Whereas delivery of a single message to the application calls receive(Message), delivering a message batch invokes
callback receive(MessageBatch). A message batch has advantages over a single message:

- Resources such as locks, threads etc are created/locked only once per batch rather than once per message

- The header/payload ratio is better for batches




Application batches
-------------------

TriCache uses Data to exchange single GET, PUT, ACK and BACKUP operations. However, whenever possible, multiple
operations are batched into a DataBatch instance and sent to the same destination. This is what's called
_application batching_, which is used by TriCache in addition to message batching (done by JGroups), to reduce the
amount of data sent across the network and to bulk-process many operations in one go.

A DataBatch always has the same destination, but may contain many different operations. When a DataBatch is received,
it is processed as follows (TriCache.process(DataBatch)):

- Iterate over the DataBatch:
  - On a PUT: increment puts
  - On a GET: get the value and set it _in place_, ie. at the same index of the data batch where the PUT was
            : replace the PUT operation with BACKUP
  - On an ACK: complete the future, so the caller is unblocked, and null the data at the fiven index
  - On a BACKUP: update the local hashmap and send an ACK back to the original sender, plus null the element at the
                 given index

- At this point, ACKs and BACKUPs have been processed and nulled in the data batch
- If there were any PUTs: add the data batch to the put-queue (see below)
- If there were any GETs: send all ACKs in the batch back to the sender using the same batch (but skipping any operation
  other then ACK).

- Example: a data batch of 2 PUTs, 2 ACKs, 2 BACKUPs and 3 GETs will be processed as follows:

  [0] PUT     -->     [0] BACKUP
  [1] PUT     -->     [1] BACKUP
  [2] ACK     -->     [2]  -
  [3] ACK     -->     [3]  -
  [4] BACKUP  -->     [4]  - // ACK already sent
  [5] BACKUP  -->     [5]  - // ACK already sent
  [6] GET     -->     [6] ACK
  [7] GET     -->     [7] ACK
  [8] GET     -->     [8] ACK

This will be sent as 2 batches of 2 BACKUP operations and 3 ACKs.

- Example 2: D receives a data batch of 4 GETs and 2 ACKs:

                                -- sent as -->
  [0] GET     -->     [0] ACK    --> [0] ACK
  [1] GET     -->     [1] ACK    --> [1] ACK
  [2] ACK     -->     [2]  -     --> [2] ACK
  [3] GET     -->     [3] ACK    --> [3] ACK
  [4] GET     -->     [4] ACK
  [5] ACK     -->     [5]  -

- The response batch only has 4 elements, but we didn't create a new batch, but simply skipped the null element in the
  received batch when serializing the response. The receiver side then received a data batch of 4 operations.


PUTs and ordering in put-queue
------------------------------

A PUT is applied to the local hashmap by the primary and then sent to the backup node using a BACKUP operation.

This has to be done *atomically* with respect to other PUTs, e.g. if the primary received put(x,1) and put(x,2) then
not doing this atomically could result in the following inconsistency:
- put(x,1) is received by (primary) P and is processed in thread #4
- put(x,2) is received by P and is processed in thread #5
- x is set to 2 by thread #5
- x is set to 1 by thread #4
- Thread #4 sends a BACKUP message to (backup) B. UNICAST3 assigns seqno=20 to BACKUP(x,1)
- Thread #5 sends a BACKUP message to B. UNICAST3 assigns seqno=21 to BACKUP(x,2)
- Backup node B delivers BACKUP(x,1) before BACKUP(x,2), in the order of the seqnos
==> The state is now x=2 on B and x=1 on P, which is inconsistent!

Therefore PUTs are handled only by a thread pool of fixed size 1 (with a queue), instead of acquiring a lock. This
guarantees that a PUT is only ever handled by a single thread and thus BACKUP operations are always sent in the
same order that the local hashmap is updated.

Note that BACKUPs are received by a backup node from a *single primary*, as each backup node only has one primary. These
opertions therefore don't need to acquire a lock as the stream of messages from primary to backup is ordered already
and delivered sequentially because these are regular JGroups messages.

There are also no conflicting PUTs and BACKUPs for the same key K: either a node is a primary and therefore gets all
PUTs for K, or the node is a backup for K and therefore never gets PUTs for K, only BACKUPs.

Also note that GETs don't synchronize when getting the value for a given key from the local hashmap.

